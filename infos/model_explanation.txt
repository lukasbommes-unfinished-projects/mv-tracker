###############################################
# Best models so far are:
# Oct23_09-25-34_295b671b36a0 -- Upsampled, MPEG4, Full dataset
# Oct16_09-24-32_de2d11b5ab38 -- Upsampled, MPEG4, Static cameras
#
###############################################


2019-10-23_09-25-34

model: pnet_single_branch (renamed into pnet_upsampled)
batch_size: 8 (single GPU)
epochs_trained: 160
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam + moving cam (full)
tensorboard_id: Oct23_09-25-34_295b671b36a0

Note: Normalization corrected (see Note2 of model Oct21_17-32-25_de2d11b5ab38)
Note2: The batchsize was reduced to 8 to perform training on a single GPU.

################################################

2019-10-21_17-32-25

model: pnet_single_branch
batch_size: 16
epochs_trained: 160
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam + moving cam (full)
tensorboard_id: Oct21_17-32-25_de2d11b5ab38

Note: As compared to previous runs (Oct18_04-33-16_de2d11b5ab38,
Oct17_08-16-45_de2d11b5ab38) the dataset stats differ. Previously, wrong
values (those of the static cam dataset) were used.
Note2: As figured out later the normalization was still wrong because the
padding of velocities was not considered during computation of the dataset
statistics.

################################################

Explanation of the following for runs:
Goal was to replace the previous data loader which uses batch sizes > 1 by a
loader that uses batch size 1. Batched training is achieved by accumulating
gradients in a for loop instead. This was tested in Oct20_04-37-53_de2d11b5ab38,
Oct20_04-35-02_de2d11b5ab38 and Oct21_02-31-18_de2d11b5ab38 for batch sizes
16, 8 and 4. Expected result is that Oct20_04-37-53_de2d11b5ab38 performs
similar to Oct16_09-24-32_de2d11b5ab38 as settings are the same. However, loss
of the new batching method is generally higher and also performance in the real
world test is lower. Thus, in Oct21_03-46-37_de2d11b5ab38 color augmentation was
disabled.

2019-10-21_03-46-37

model: pnet_single_branch
batch_size: 8 (accumulated gradients training)
epochs_trained: 120
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"])
dataset: static cam
tensorboard_id: Oct21_03-46-37_de2d11b5ab38

Note: Performance should be comparable with Oct16_09-24-32_de2d11b5ab38


2019-10-21_02-31-18

model: pnet_single_branch
batch_size: 4 (accumulated gradients training)
epochs_trained: 120
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam
tensorboard_id: Oct21_02-31-18_de2d11b5ab38


2019-10-20_04-37-53

model: pnet_single_branch
batch_size: 16 (accumulated gradients training)
epochs_trained: 120
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam
tensorboard_id: Oct20_04-37-53_de2d11b5ab38


2019-10-20_04-35-02

model: pnet_single_branch
batch_size: 8 (accumulated gradients training)
epochs_trained: 120
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam
tensorboard_id: Oct20_04-35-02_de2d11b5ab38

################################################


2019-10-19_19-23-37

model: pnet_single_branch
batch_size: 128 (accumulated gradients training)
epochs_trained: 160 (this model is from epoch 21)
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam
tensorboard_id: Oct19_19-23-37_de2d11b5ab38

################################################


2019-10-18_04-33-16

model: pnet_two_pools
batch_size: 16
epochs_trained: 80
initial learning rate: 1e-5
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam + moving cam (full)
tensorboard_id: Oct18_04-33-16_de2d11b5ab38

Note: Normalization of motion vectors and velocities was wrong (those of static only)

################################################


2019-10-17_08-16-45:

batch_size: 16
epochs_trained: 80
initial learning rate: 1e-5
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam + moving cam (full)
tensorboard_id: Oct17_08-16-45_de2d11b5ab38

Note: Normalization of motion vectors and velocities was wrong (those of static only)


################################################


2019-10-16_09-24-32:

batch_size: 16
epochs_trained: 160
initial learning rate: 1e-4
scheduler: every 40 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"]), color_augmentation(scale=1.0)
dataset: static cam only
tensorboard_id: Oct16_09-24-32_de2d11b5ab38
                Oct19_05-34-00_de2d11b5ab38 (same settings, run 2)

Notes: Added color augmentation and changed learning rate schedule.


################################################


14_10_2019_01.pth

batch size: 16
epochs_trained: 80
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct14_08-17-23_de2d11b5ab38

Notes: Added additional instrumentation for weight to update ration and
       mean IoU. Loss curve should be similar to 12_10_2019_03.pth.
       Also added num_boxes_mask to only consider actual boxes and not the
       padding.


################################################


13_10_2019_02.pth

batch size: 16
epochs_trained: 150
initial learning rate: 1e-4
scheduler: every 30 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct13_03-57-17_de2d11b5ab38


################################################

12_10_2019_03.pth

batch size: 16
epochs_trained: 80
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: no scale, random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct12_12-27-39_de2d11b5ab38


12_10_2019_02.pth

batch size: 16
epochs_trained: 80
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: random_scale([300, 400, 500, 600, 700, 800, 900], maxsize=1920), random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct12_08-24-15_de2d11b5ab38


################################################

12_10_2019_90epochs.pth

batch size: 32
epochs_trained: 90
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: random_scale([300, 400, 500, 600], maxsize=1000), random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct11_17-00-08_de2d11b5ab38


11_10_2019_148epochs.pth

batch size: 16
epochs_trained: 148
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: random_scale([300, 400, 500, 600], maxsize=1000), random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct11_13-20-34_de2d11b5ab38


11_10_2019_36epochs.pth

batch size: 4
epochs_trained: 36
initial learning rate: 1e-4
scheduler: every 10 epochs reduction by factor 10
augmentation: random_scale([300, 400, 500, 600], maxsize=1000), random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct11_12-04-26_de2d11b5ab38


11_10_2019_90epochs_diverged.pth

batch size: 4
epochs_trained: 90
initial learning rate: 1e-4
scheduler: no
augmentation: random_scale([300, 400, 500, 600], maxsize=1000), random_flip(["x", "y"])
dataset: static cam only
tensorboard_id: Oct11_09-41-58_de2d11b5ab38


################################################

.pth

batch size: 32
epochs_trained:
initial learning rate: 1e-4
scheduler: no
augmentation: no
dataset: static cam only
tensorboard_id:


09_10_2019.pth

batch size: 8 (?)
epochs_trained:
initial learning rate: 1e-4
scheduler: no
augmentation: no
dataset: static cam only
tensorboard_id:



###############################################
Empirical Results:
- Batch size 4:
- Batch Size 16: is a good compromise, batch size 32 leads to
- Batch size 32:

Data augmentation:
- no augmentation makes the network memorize the train data, train loss decreases rapidlz, but validation loss stays high -> overfitting occurs
- random flipping: increases generalization performance of the network
- random flipping + random scaling: prevents overfitting, but leads to low final validation and training accuracy
-random scaling alone:
